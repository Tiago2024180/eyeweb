#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
===========================================
Eye Web Updater ‚Äî Script Principal (Fase 1)
===========================================

Este script √© respons√°vel por:
1. Gerar/processar dados de breaches (sint√©ticos ou reais)
2. Normalizar emails e gerar hashes SHA-256
3. Particionar os dados por prefixo do hash
4. Comprimir em formato Apache Parquet (Snappy)
5. Fazer upload autom√°tico para o Hugging Face Datasets

Execu√ß√£o:
    python updater.py

Vari√°veis de Ambiente Necess√°rias:
    HF_TOKEN: Token do Hugging Face com permiss√£o de escrita
    HF_DATASET_REPO: Nome do reposit√≥rio (username/repo-name)

Autor: Eye Web PAP Project
Data: Janeiro 2026
"""

import os
import sys
import hashlib
import random
import string
import logging
from datetime import datetime
from typing import Dict, List, Optional

import pandas as pd
from tqdm import tqdm
from huggingface_hub import HfApi, login

# Importar configura√ß√µes do projeto
from config import (
    HF_TOKEN,
    HF_DATASET_REPO,
    HF_BRANCH,
    PREFIX_LENGTH,
    HEX_CHARS,
    DATA_DIR,
    OUTPUT_DIR,
    PARQUET_COMPRESSION,
    SYNTHETIC_RECORDS,
    SAMPLE_BREACHES,
    LOG_LEVEL,
    LOG_DATE_FORMAT,
    validate_config,
    print_config_summary
)


# ===========================================
# CONFIGURA√á√ÉO DE LOGGING
# ===========================================

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format=f"%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt=LOG_DATE_FORMAT
)
logger = logging.getLogger(__name__)


# ===========================================
# FUN√á√ïES UTILIT√ÅRIAS
# ===========================================

def normalize_email(email: str) -> str:
    """
    Normaliza um email para garantir consist√™ncia no hashing.
    
    Opera√ß√µes realizadas:
    - Converte para min√∫sculas
    - Remove espa√ßos em branco
    - Remove pontos do username do Gmail (opcional)
    
    Args:
        email: Email original a normalizar
        
    Returns:
        str: Email normalizado
        
    Exemplo:
        >>> normalize_email("  Teste.User@Gmail.com  ")
        "testeuser@gmail.com"
    """
    # Remover espa√ßos e converter para min√∫sculas
    email = email.strip().lower()
    
    # Tratamento especial para Gmail (pontos s√£o ignorados)
    # Exemplo: john.doe@gmail.com == johndoe@gmail.com
    if "@gmail.com" in email:
        username, domain = email.split("@")
        username = username.replace(".", "")
        # Remover sufixo + (aliases do Gmail)
        username = username.split("+")[0]
        email = f"{username}@{domain}"
    
    return email


def generate_sha256_hash(text: str) -> str:
    """
    Gera o hash SHA-256 de um texto.
    
    Args:
        text: Texto a ser hasheado (ex: email normalizado)
        
    Returns:
        str: Hash SHA-256 em hexadecimal (64 caracteres)
        
    Exemplo:
        >>> generate_sha256_hash("teste@exemplo.com")
        "a1b2c3d4e5f6..."  # 64 chars hexadecimais
    """
    # Codificar o texto em bytes (UTF-8)
    text_bytes = text.encode('utf-8')
    
    # Criar objeto hash SHA-256
    hash_object = hashlib.sha256(text_bytes)
    
    # Retornar representa√ß√£o hexadecimal
    return hash_object.hexdigest()


def get_hash_prefix(hash_value: str, length: int = PREFIX_LENGTH) -> str:
    """
    Extrai o prefixo de um hash para particionamento.
    
    Args:
        hash_value: Hash SHA-256 completo
        length: N√∫mero de caracteres do prefixo
        
    Returns:
        str: Prefixo do hash (ex: "ef", "a3", "00")
        
    Exemplo:
        >>> get_hash_prefix("ef7241abc...", 2)
        "ef"
    """
    return hash_value[:length].lower()


# ===========================================
# GERA√á√ÉO DE DADOS SINT√âTICOS
# ===========================================

def generate_random_email() -> str:
    """
    Gera um email aleat√≥rio para dados de teste.
    
    Returns:
        str: Email aleat√≥rio no formato user123@domain.com
    """
    # Dom√≠nios comuns para simular
    domains = [
        "gmail.com", "hotmail.com", "yahoo.com", "outlook.com",
        "example.com", "test.org", "demo.net", "sample.io"
    ]
    
    # Gerar username aleat√≥rio
    username_length = random.randint(6, 12)
    username = ''.join(random.choices(string.ascii_lowercase + string.digits, k=username_length))
    
    # Selecionar dom√≠nio aleat√≥rio
    domain = random.choice(domains)
    
    return f"{username}@{domain}"


def generate_synthetic_dataset(num_records: int = SYNTHETIC_RECORDS) -> pd.DataFrame:
    """
    Gera um dataset sint√©tico de breaches para testes/demonstra√ß√£o.
    
    NOTA: Em produ√ß√£o, esta fun√ß√£o seria substitu√≠da por uma que
    obt√©m dados reais de APIs p√∫blicas (ex: HIBP API, se dispon√≠vel).
    
    Args:
        num_records: N√∫mero de registos a gerar
        
    Returns:
        pd.DataFrame: Dataset com colunas:
            - hash: Hash SHA-256 do email
            - prefix: Prefixo do hash (para particionamento)
            - breach_name: Nome do breach
            - breach_date: Data do breach
            - data_classes: Tipos de dados expostos
    """
    logger.info(f"üîÑ A gerar {num_records:,} registos sint√©ticos...")
    
    records = []
    
    # Usar tqdm para mostrar progresso
    for _ in tqdm(range(num_records), desc="Gerando dados", unit="registos"):
        # Gerar email aleat√≥rio
        email = generate_random_email()
        
        # Normalizar e gerar hash
        normalized_email = normalize_email(email)
        email_hash = generate_sha256_hash(normalized_email)
        prefix = get_hash_prefix(email_hash)
        
        # Selecionar breach aleat√≥rio
        breach = random.choice(SAMPLE_BREACHES)
        
        # Criar registo
        record = {
            "hash": email_hash,
            "prefix": prefix,
            "breach_name": breach["name"],
            "breach_date": breach["date"],
            "data_classes": ",".join(breach["data_classes"])  # Serializar lista como string
        }
        
        records.append(record)
    
    # Criar DataFrame
    df = pd.DataFrame(records)
    
    logger.info(f"‚úÖ Dataset gerado com {len(df):,} registos")
    logger.info(f"   Colunas: {list(df.columns)}")
    logger.info(f"   Prefixos √∫nicos: {df['prefix'].nunique()}")
    
    return df


# ===========================================
# PARTICIONAMENTO DE DADOS
# ===========================================

def partition_dataset(df: pd.DataFrame, output_dir: str = OUTPUT_DIR) -> Dict[str, str]:
    """
    Particiona o dataset por prefixo do hash e guarda em ficheiros Parquet.
    
    Cada parti√ß√£o √© guardada num ficheiro separado (ex: ef.parquet, 00.parquet).
    Isto permite que a API leia apenas a parti√ß√£o necess√°ria, otimizando
    o uso de mem√≥ria no Render Free Tier (512MB).
    
    Args:
        df: DataFrame com os dados de breaches
        output_dir: Diret√≥rio onde guardar os ficheiros Parquet
        
    Returns:
        Dict[str, str]: Mapeamento {prefixo: caminho_ficheiro}
        
    Estrutura de Sa√≠da:
        output_dir/
        ‚îú‚îÄ‚îÄ 00.parquet
        ‚îú‚îÄ‚îÄ 01.parquet
        ‚îú‚îÄ‚îÄ ...
        ‚îú‚îÄ‚îÄ fe.parquet
        ‚îî‚îÄ‚îÄ ff.parquet
    """
    logger.info(f"üìÇ A particionar dataset por prefixo (comprimento={PREFIX_LENGTH})...")
    
    # Criar diret√≥rio de sa√≠da se n√£o existir
    os.makedirs(output_dir, exist_ok=True)
    
    # Agrupar por prefixo
    grouped = df.groupby("prefix")
    
    # Dicion√°rio para guardar caminhos dos ficheiros
    partition_files = {}
    
    # Iterar por cada grupo (prefixo)
    for prefix, group_df in tqdm(grouped, desc="Particionando", unit="parti√ß√µes"):
        # Definir caminho do ficheiro
        file_path = os.path.join(output_dir, f"{prefix}.parquet")
        
        # Remover coluna de prefixo (j√° est√° no nome do ficheiro)
        partition_df = group_df.drop(columns=["prefix"])
        
        # Guardar em formato Parquet com compress√£o
        partition_df.to_parquet(
            file_path,
            engine="pyarrow",
            compression=PARQUET_COMPRESSION,
            index=False
        )
        
        partition_files[prefix] = file_path
        
        logger.debug(f"   {prefix}.parquet: {len(partition_df):,} registos")
    
    logger.info(f"‚úÖ Particionamento completo!")
    logger.info(f"   Total de parti√ß√µes: {len(partition_files)}")
    logger.info(f"   Diret√≥rio: {output_dir}")
    
    return partition_files


# ===========================================
# UPLOAD PARA HUGGING FACE
# ===========================================

def upload_to_huggingface(
    output_dir: str = OUTPUT_DIR,
    repo_id: str = HF_DATASET_REPO,
    token: str = HF_TOKEN
) -> bool:
    """
    Faz upload da pasta completa de ficheiros Parquet para o Hugging Face.
    
    OTIMIZADO: Usa upload_folder para fazer tudo num √∫nico commit,
    evitando o limite de rate (128 commits/hora no plano gratuito).
    
    Args:
        output_dir: Diret√≥rio local com os ficheiros Parquet
        repo_id: ID do reposit√≥rio no formato "username/repo-name"
        token: Token de autentica√ß√£o do Hugging Face
        
    Returns:
        bool: True se o upload foi bem sucedido, False caso contr√°rio
        
    Estrutura no Hugging Face:
        repo/
        ‚îî‚îÄ‚îÄ data/
            ‚îú‚îÄ‚îÄ 00.parquet
            ‚îú‚îÄ‚îÄ 01.parquet
            ‚îî‚îÄ‚îÄ ...
    """
    logger.info(f"‚òÅÔ∏è A iniciar upload para Hugging Face...")
    logger.info(f"   Reposit√≥rio: {repo_id}")
    logger.info(f"   Pasta local: {output_dir}")
    
    try:
        # Autenticar no Hugging Face
        login(token=token)
        logger.info("   ‚úÖ Autentica√ß√£o bem sucedida")
        
        # Criar inst√¢ncia da API
        api = HfApi()
        
        # Verificar se o reposit√≥rio existe, se n√£o, criar
        try:
            api.repo_info(repo_id=repo_id, repo_type="dataset")
            logger.info(f"   ‚úÖ Reposit√≥rio encontrado")
        except Exception:
            logger.info(f"   üìÅ A criar novo reposit√≥rio...")
            api.create_repo(
                repo_id=repo_id,
                repo_type="dataset",
                private=False,  # P√∫blico para que a API possa aceder
                exist_ok=True
            )
            logger.info(f"   ‚úÖ Reposit√≥rio criado")
        
        # Contar ficheiros a enviar
        parquet_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]
        logger.info(f"   üì¶ A preparar upload de {len(parquet_files)} ficheiros...")
        
        # Upload da pasta completa num √∫nico commit
        # Isto evita o limite de 128 commits/hora!
        api.upload_folder(
            folder_path=output_dir,
            path_in_repo="data",
            repo_id=repo_id,
            repo_type="dataset",
            commit_message=f"Update dataset: {len(parquet_files)} partitions"
        )
        
        logger.info(f"‚úÖ Upload completo! {len(parquet_files)} ficheiros enviados.")
        logger.info(f"   üîó https://huggingface.co/datasets/{repo_id}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro no upload: {str(e)}")
        return False


# ===========================================
# CRIA√á√ÉO DO FICHEIRO DE METADADOS
# ===========================================

def create_metadata_file(partition_files: Dict[str, str], output_dir: str = OUTPUT_DIR) -> str:
    """
    Cria um ficheiro JSON com metadados do dataset.
    
    Este ficheiro ajuda a API a saber quais parti√ß√µes existem
    e cont√©m estat√≠sticas √∫teis.
    
    Args:
        partition_files: Dicion√°rio {prefixo: caminho_local}
        output_dir: Diret√≥rio de sa√≠da
        
    Returns:
        str: Caminho do ficheiro de metadados
    """
    import json
    
    # Calcular estat√≠sticas
    total_records = 0
    partition_stats = {}
    
    for prefix, file_path in partition_files.items():
        df = pd.read_parquet(file_path)
        count = len(df)
        total_records += count
        partition_stats[prefix] = count
    
    # Criar metadados
    metadata = {
        "version": "1.0.0",
        "generated_at": datetime.now().isoformat(),
        "prefix_length": PREFIX_LENGTH,
        "compression": PARQUET_COMPRESSION,
        "total_records": total_records,
        "total_partitions": len(partition_files),
        "partitions": partition_stats
    }
    
    # Guardar ficheiro
    metadata_path = os.path.join(output_dir, "metadata.json")
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üìã Metadados guardados em: {metadata_path}")
    
    return metadata_path


def upload_metadata(metadata_path: str, repo_id: str = HF_DATASET_REPO, token: str = HF_TOKEN):
    """
    Faz upload do ficheiro de metadados para o Hugging Face.
    """
    api = HfApi()
    
    api.upload_file(
        path_or_fileobj=metadata_path,
        path_in_repo="data/metadata.json",
        repo_id=repo_id,
        repo_type="dataset",
        commit_message="Update metadata.json"
    )
    
    logger.info("üìã Metadados enviados para o Hugging Face")


# ===========================================
# LIMPEZA DE FICHEIROS TEMPOR√ÅRIOS
# ===========================================

def cleanup_temp_files(output_dir: str = OUTPUT_DIR):
    """
    Remove ficheiros tempor√°rios ap√≥s o upload.
    
    Args:
        output_dir: Diret√≥rio a limpar
    """
    import shutil
    
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
        logger.info(f"üßπ Ficheiros tempor√°rios removidos: {output_dir}")


# ===========================================
# FUN√á√ÉO PRINCIPAL
# ===========================================

def main():
    """
    Fun√ß√£o principal que orquestra todo o processo de atualiza√ß√£o.
    
    Fluxo:
    1. Validar configura√ß√£o
    2. Gerar/obter dados
    3. Particionar por prefixo
    4. Criar metadados
    5. Upload para Hugging Face
    6. Limpeza
    """
    print("\n" + "="*60)
    print("üëÅÔ∏è  EYE WEB UPDATER ‚Äî Breach Dataset Pipeline")
    print("="*60)
    
    # Mostrar configura√ß√£o
    print_config_summary()
    
    # 1. Validar configura√ß√£o
    logger.info("üîç A validar configura√ß√£o...")
    is_valid, message = validate_config()
    
    if not is_valid:
        logger.error(f"‚ùå Configura√ß√£o inv√°lida:\n{message}")
        sys.exit(1)
    
    logger.info(message)
    
    # 2. Gerar dados sint√©ticos (em produ√ß√£o, seria obter dados reais)
    logger.info("\n" + "-"*40)
    logger.info("FASE 1: Gera√ß√£o de Dados")
    logger.info("-"*40)
    
    df = generate_synthetic_dataset()
    
    # 3. Particionar dataset
    logger.info("\n" + "-"*40)
    logger.info("FASE 2: Particionamento")
    logger.info("-"*40)
    
    partition_files = partition_dataset(df)
    
    # 4. Criar metadados
    logger.info("\n" + "-"*40)
    logger.info("FASE 3: Metadados")
    logger.info("-"*40)
    
    metadata_path = create_metadata_file(partition_files)
    
    # 5. Upload para Hugging Face
    logger.info("\n" + "-"*40)
    logger.info("FASE 4: Upload")
    logger.info("-"*40)
    
    # Usa upload_folder para evitar limite de rate (128 commits/hora)
    success = upload_to_huggingface(OUTPUT_DIR)
    
    if success:
        upload_metadata(metadata_path)
    
    # 6. Limpeza (opcional - comentar se quiseres manter os ficheiros)
    # cleanup_temp_files()
    
    # Resumo final
    print("\n" + "="*60)
    if success:
        print("‚úÖ ATUALIZA√á√ÉO CONCLU√çDA COM SUCESSO!")
        print(f"   Dataset dispon√≠vel em:")
        print(f"   https://huggingface.co/datasets/{HF_DATASET_REPO}")
    else:
        print("‚ùå ATUALIZA√á√ÉO FALHOU - Verificar logs acima")
    print("="*60 + "\n")
    
    return 0 if success else 1


# ===========================================
# PONTO DE ENTRADA
# ===========================================

if __name__ == "__main__":
    sys.exit(main())
