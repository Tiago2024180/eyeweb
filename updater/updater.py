#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
===========================================
Eye Web Updater ‚Äî Script Principal (Fase 1)
===========================================

Este script √© respons√°vel por:
1. Gerar/processar dados de breaches (sint√©ticos ou reais)
2. Normalizar emails e gerar hashes SHA-256
3. Particionar os dados por prefixo do hash
4. Comprimir em formato Apache Parquet (Snappy)
5. Fazer upload autom√°tico para o Hugging Face Datasets

Execu√ß√£o:
    python updater.py

Vari√°veis de Ambiente Necess√°rias:
    HF_TOKEN: Token do Hugging Face com permiss√£o de escrita
    HF_DATASET_REPO: Nome do reposit√≥rio (username/repo-name)

Autor: Eye Web PAP Project
Data: Janeiro 2026
"""

import os
import sys
import hashlib
import random
import string
import logging
from datetime import datetime
from typing import Dict, List, Optional

import pandas as pd
from tqdm import tqdm
from huggingface_hub import HfApi, login

# Importar configura√ß√µes do projeto
from config import (
    HF_TOKEN,
    HF_DATASET_REPO,
    HF_BRANCH,
    PREFIX_LENGTH,
    HEX_CHARS,
    DATA_DIR,
    OUTPUT_DIR,
    PARQUET_COMPRESSION,
    SYNTHETIC_EMAIL_RECORDS,
    SYNTHETIC_PHONE_RECORDS,
    SAMPLE_BREACHES,
    COUNTRY_PHONE_CODES,
    DATA_TYPES,
    LOG_LEVEL,
    LOG_DATE_FORMAT,
    validate_config,
    print_config_summary
)


# ===========================================
# CONFIGURA√á√ÉO DE LOGGING
# ===========================================

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format=f"%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt=LOG_DATE_FORMAT
)
logger = logging.getLogger(__name__)


# ===========================================
# FUN√á√ïES UTILIT√ÅRIAS
# ===========================================

def normalize_email(email: str) -> str:
    """
    Normaliza um email para garantir consist√™ncia no hashing.
    
    Opera√ß√µes realizadas:
    - Converte para min√∫sculas
    - Remove espa√ßos em branco
    - Remove pontos do username do Gmail (opcional)
    
    Args:
        email: Email original a normalizar
        
    Returns:
        str: Email normalizado
        
    Exemplo:
        >>> normalize_email("  Teste.User@Gmail.com  ")
        "testeuser@gmail.com"
    """
    # Remover espa√ßos e converter para min√∫sculas
    email = email.strip().lower()
    
    # Tratamento especial para Gmail (pontos s√£o ignorados)
    # Exemplo: john.doe@gmail.com == johndoe@gmail.com
    if "@gmail.com" in email:
        username, domain = email.split("@")
        username = username.replace(".", "")
        # Remover sufixo + (aliases do Gmail)
        username = username.split("+")[0]
        email = f"{username}@{domain}"
    
    return email


def normalize_phone(phone: str, country_code: str = "") -> str:
    """
    Normaliza um n√∫mero de telefone para garantir consist√™ncia no hashing.
    
    IMPORTANTE para K-Anonymity: O n√∫mero completo (com c√≥digo de pa√≠s)
    deve ser normalizado antes de gerar o hash. O cliente (frontend)
    deve usar EXATAMENTE a mesma l√≥gica de normaliza√ß√£o.
    
    Opera√ß√µes realizadas:
    - Remove todos os espa√ßos, h√≠fens, par√™nteses
    - Garante que o c√≥digo de pa√≠s est√° presente
    - Garante formato: +XXXYYYYYYYYY (sem separadores)
    
    Args:
        phone: N√∫mero de telefone (pode ter formata√ß√£o)
        country_code: C√≥digo do pa√≠s (ex: "+351")
        
    Returns:
        str: Telefone normalizado no formato +XXXYYYYYYYYY
        
    Exemplo:
        >>> normalize_phone("912 341 801", "+351")
        "+351912341801"
        >>> normalize_phone("+351 912-341-801")
        "+351912341801"
    """
    # Remover todos os caracteres n√£o num√©ricos, exceto o + inicial
    cleaned = ""
    for i, char in enumerate(phone):
        if char == "+" and i == 0:
            cleaned += char
        elif char.isdigit():
            cleaned += char
    
    # Se n√£o come√ßa com +, adicionar o c√≥digo de pa√≠s
    if not cleaned.startswith("+"):
        if country_code:
            # Garantir que o c√≥digo de pa√≠s come√ßa com +
            if not country_code.startswith("+"):
                country_code = "+" + country_code
            cleaned = country_code + cleaned
        else:
            # Se n√£o h√° c√≥digo de pa√≠s, assumir +351 (Portugal) como default
            cleaned = "+351" + cleaned
    
    return cleaned


def generate_sha256_hash(text: str) -> str:
    """
    Gera o hash SHA-256 de um texto.
    
    Args:
        text: Texto a ser hasheado (ex: email normalizado)
        
    Returns:
        str: Hash SHA-256 em hexadecimal (64 caracteres)
        
    Exemplo:
        >>> generate_sha256_hash("teste@exemplo.com")
        "a1b2c3d4e5f6..."  # 64 chars hexadecimais
    """
    # Codificar o texto em bytes (UTF-8)
    text_bytes = text.encode('utf-8')
    
    # Criar objeto hash SHA-256
    hash_object = hashlib.sha256(text_bytes)
    
    # Retornar representa√ß√£o hexadecimal
    return hash_object.hexdigest()


def get_hash_prefix(hash_value: str, length: int = PREFIX_LENGTH) -> str:
    """
    Extrai o prefixo de um hash para particionamento.
    
    Args:
        hash_value: Hash SHA-256 completo
        length: N√∫mero de caracteres do prefixo
        
    Returns:
        str: Prefixo do hash (ex: "ef", "a3", "00")
        
    Exemplo:
        >>> get_hash_prefix("ef7241abc...", 2)
        "ef"
    """
    return hash_value[:length].lower()


# ===========================================
# GERA√á√ÉO DE DADOS SINT√âTICOS
# ===========================================

def generate_random_email() -> str:
    """
    Gera um email aleat√≥rio para dados de teste.
    
    Returns:
        str: Email aleat√≥rio no formato user123@domain.com
    """
    # Dom√≠nios comuns para simular
    domains = [
        "gmail.com", "hotmail.com", "yahoo.com", "outlook.com",
        "example.com", "test.org", "demo.net", "sample.io",
        "protonmail.com", "icloud.com", "live.com", "mail.com"
    ]
    
    # Gerar username aleat√≥rio
    username_length = random.randint(6, 12)
    username = ''.join(random.choices(string.ascii_lowercase + string.digits, k=username_length))
    
    # Selecionar dom√≠nio aleat√≥rio
    domain = random.choice(domains)
    
    return f"{username}@{domain}"


def generate_random_phone() -> tuple:
    """
    Gera um n√∫mero de telefone aleat√≥rio com c√≥digo de pa√≠s.
    
    Returns:
        tuple: (telefone_completo_normalizado, c√≥digo_pa√≠s)
        
    Exemplo:
        >>> generate_random_phone()
        ("+351912345678", "+351")
    """
    # Selecionar c√≥digo de pa√≠s aleat√≥rio
    country_code = random.choice(list(COUNTRY_PHONE_CODES.keys()))
    country_name, min_digits, max_digits = COUNTRY_PHONE_CODES[country_code]
    
    # Gerar n√∫mero com o comprimento correto para o pa√≠s
    num_digits = random.randint(min_digits, max_digits)
    
    # Primeiro d√≠gito n√£o pode ser 0 para a maioria dos pa√≠ses
    first_digit = random.choice("123456789")
    remaining_digits = ''.join(random.choices(string.digits, k=num_digits - 1))
    
    phone_number = first_digit + remaining_digits
    
    # Normalizar (c√≥digo + n√∫mero, sem espa√ßos)
    normalized = f"{country_code}{phone_number}"
    
    return normalized, country_code


# ===========================================
# DADOS DE TESTE CONHECIDOS (para verifica√ß√£o)
# ===========================================

# Emails que SABEMOS que est√£o na base de dados
# Usar estes para testar se o sistema deteta breaches corretamente
TEST_EMAILS = [
    ("leaked@test.com", "TestBreach2024", "2024-01-15"),
    ("hacked@example.com", "ExampleHack2023", "2023-06-20"),
    ("breach@demo.com", "DemoLeak2024", "2024-03-10"),
    ("exposed@sample.com", "SampleExposure2023", "2023-11-05"),
    ("pwned@eyeweb.test", "EyeWebTest2024", "2024-07-01"),
]

# Telefones que SABEMOS que est√£o na base de dados
# Formato: (n√∫mero, c√≥digo_pa√≠s, breach_name, breach_date)
# IMPORTANTE: Usar estes n√∫meros EXATAMENTE para testar
TEST_PHONES = [
    ("+351912345678", "DataBreach2024", "2024-01-20"),      # Portugal
    ("+351961234567", "SocialMediaBreach2024", "2024-05-18"),  # Portugal
    ("+34612345678", "EcommerceHack2023", "2023-12-03"),    # Espanha
    ("+44712345678", "GamingDB2024", "2024-02-28"),         # Reino Unido (exemplo: 07123456780 -> +447123456780)
    ("+5511912345678", "HealthcareExposure2023", "2023-07-14"),  # Brasil
]


def generate_synthetic_dataset() -> pd.DataFrame:
    """
    Gera um dataset sint√©tico de breaches para testes/demonstra√ß√£o.
    
    NOVA ESTRUTURA com:
    - Suporte para emails E telefones
    - Campos booleanos individuais para cada tipo de dado exposto
    - Coluna 'type' para distinguir email de phone
    
    NOTA: Em produ√ß√£o, esta fun√ß√£o seria substitu√≠da por uma que
    obt√©m dados reais de APIs p√∫blicas (ex: HIBP API, se dispon√≠vel).
        
    Returns:
        pd.DataFrame: Dataset com colunas:
            - hash: Hash SHA-256 do email/phone
            - type: "email" ou "phone"
            - prefix: Prefixo do hash (para particionamento)
            - breach_name: Nome do breach
            - breach_date: Data do breach
            - has_password: Boolean
            - has_ip: Boolean
            - has_username: Boolean
            - has_credit_card: Boolean
            - has_history: Boolean
    """
    total_records = SYNTHETIC_EMAIL_RECORDS + SYNTHETIC_PHONE_RECORDS
    logger.info(f"üîÑ A gerar {total_records:,} registos sint√©ticos...")
    logger.info(f"   üìß Emails: {SYNTHETIC_EMAIL_RECORDS:,}")
    logger.info(f"   üì± Telefones: {SYNTHETIC_PHONE_RECORDS:,}")
    
    records = []
    
    # ===========================================
    # PRIMEIRO: Adicionar dados de teste CONHECIDOS
    # ===========================================
    logger.info("üìå A adicionar emails de teste conhecidos...")
    for email, breach_name, breach_date in TEST_EMAILS:
        normalized = normalize_email(email)
        data_hash = generate_sha256_hash(normalized)
        prefix = get_hash_prefix(data_hash)
        
        record = {
            "hash": data_hash,
            "type": "email",
            "prefix": prefix,
            "breach_name": breach_name,
            "breach_date": breach_date,
            "has_password": True,
            "has_ip": True,
            "has_username": True,
            "has_credit_card": False,
            "has_history": True
        }
        records.append(record)
        logger.debug(f"   ‚úÖ {email} -> prefix={prefix}")
    
    logger.info("üìå A adicionar telefones de teste conhecidos...")
    for phone, breach_name, breach_date in TEST_PHONES:
        # O telefone j√° est√° normalizado na lista
        data_hash = generate_sha256_hash(phone)
        prefix = get_hash_prefix(data_hash)
        
        record = {
            "hash": data_hash,
            "type": "phone",
            "prefix": prefix,
            "breach_name": breach_name,
            "breach_date": breach_date,
            "has_password": True,
            "has_ip": True,
            "has_username": False,
            "has_credit_card": True,
            "has_history": True
        }
        records.append(record)
        logger.debug(f"   ‚úÖ {phone} -> prefix={prefix}")
    
    logger.info(f"   üìå Total de dados de teste: {len(TEST_EMAILS)} emails + {len(TEST_PHONES)} telefones")
    
    # ===========================================
    # DEPOIS: Gerar dados aleat√≥rios
    # ===========================================
    
    # === GERAR REGISTOS DE EMAIL ===
    logger.info("üìß A gerar registos de email aleat√≥rios...")
    for _ in tqdm(range(SYNTHETIC_EMAIL_RECORDS), desc="Emails", unit="registos"):
        # Gerar email aleat√≥rio
        email = generate_random_email()
        
        # Normalizar e gerar hash
        normalized = normalize_email(email)
        data_hash = generate_sha256_hash(normalized)
        prefix = get_hash_prefix(data_hash)
        
        # Selecionar breach aleat√≥rio
        breach = random.choice(SAMPLE_BREACHES)
        
        # Criar registo com a NOVA ESTRUTURA
        record = {
            "hash": data_hash,
            "type": "email",
            "prefix": prefix,
            "breach_name": breach["name"],
            "breach_date": breach["date"],
            "has_password": breach["has_password"],
            "has_ip": breach["has_ip"],
            "has_username": breach["has_username"],
            "has_credit_card": breach["has_credit_card"],
            "has_history": breach["has_history"]
        }
        
        records.append(record)
    
    # === GERAR REGISTOS DE TELEFONE ===
    logger.info("üì± A gerar registos de telefone aleat√≥rios...")
    for _ in tqdm(range(SYNTHETIC_PHONE_RECORDS), desc="Telefones", unit="registos"):
        # Gerar telefone aleat√≥rio
        phone, country_code = generate_random_phone()
        
        # O telefone j√° vem normalizado da fun√ß√£o
        data_hash = generate_sha256_hash(phone)
        prefix = get_hash_prefix(data_hash)
        
        # Selecionar breach aleat√≥rio
        breach = random.choice(SAMPLE_BREACHES)
        
        # Criar registo com a NOVA ESTRUTURA
        record = {
            "hash": data_hash,
            "type": "phone",
            "prefix": prefix,
            "breach_name": breach["name"],
            "breach_date": breach["date"],
            "has_password": breach["has_password"],
            "has_ip": breach["has_ip"],
            "has_username": breach["has_username"],
            "has_credit_card": breach["has_credit_card"],
            "has_history": breach["has_history"]
        }
        
        records.append(record)
    
    # Criar DataFrame
    df = pd.DataFrame(records)
    
    # Estat√≠sticas finais
    logger.info(f"‚úÖ Dataset gerado com {len(df):,} registos")
    logger.info(f"   Colunas: {list(df.columns)}")
    logger.info(f"   Prefixos √∫nicos: {df['prefix'].nunique()}")
    logger.info(f"   Emails: {len(df[df['type'] == 'email']):,}")
    logger.info(f"   Telefones: {len(df[df['type'] == 'phone']):,}")
    
    return df


# ===========================================
# PARTICIONAMENTO DE DADOS
# ===========================================

def partition_dataset(df: pd.DataFrame, output_dir: str = OUTPUT_DIR) -> Dict[str, str]:
    """
    Particiona o dataset por prefixo do hash e guarda em ficheiros Parquet.
    
    Cada parti√ß√£o √© guardada num ficheiro separado (ex: ef.parquet, 00.parquet).
    Isto permite que a API leia apenas a parti√ß√£o necess√°ria, otimizando
    o uso de mem√≥ria no Render Free Tier (512MB).
    
    Args:
        df: DataFrame com os dados de breaches
        output_dir: Diret√≥rio onde guardar os ficheiros Parquet
        
    Returns:
        Dict[str, str]: Mapeamento {prefixo: caminho_ficheiro}
        
    Estrutura de Sa√≠da:
        output_dir/
        ‚îú‚îÄ‚îÄ 00.parquet
        ‚îú‚îÄ‚îÄ 01.parquet
        ‚îú‚îÄ‚îÄ ...
        ‚îú‚îÄ‚îÄ fe.parquet
        ‚îî‚îÄ‚îÄ ff.parquet
    """
    logger.info(f"üìÇ A particionar dataset por prefixo (comprimento={PREFIX_LENGTH})...")
    
    # Criar diret√≥rio de sa√≠da se n√£o existir
    os.makedirs(output_dir, exist_ok=True)
    
    # Agrupar por prefixo
    grouped = df.groupby("prefix")
    
    # Dicion√°rio para guardar caminhos dos ficheiros
    partition_files = {}
    
    # Iterar por cada grupo (prefixo)
    for prefix, group_df in tqdm(grouped, desc="Particionando", unit="parti√ß√µes"):
        # Definir caminho do ficheiro
        file_path = os.path.join(output_dir, f"{prefix}.parquet")
        
        # Remover coluna de prefixo (j√° est√° no nome do ficheiro)
        partition_df = group_df.drop(columns=["prefix"])
        
        # Guardar em formato Parquet com compress√£o
        partition_df.to_parquet(
            file_path,
            engine="pyarrow",
            compression=PARQUET_COMPRESSION,
            index=False
        )
        
        partition_files[prefix] = file_path
        
        logger.debug(f"   {prefix}.parquet: {len(partition_df):,} registos")
    
    logger.info(f"‚úÖ Particionamento completo!")
    logger.info(f"   Total de parti√ß√µes: {len(partition_files)}")
    logger.info(f"   Diret√≥rio: {output_dir}")
    
    return partition_files


# ===========================================
# UPLOAD PARA HUGGING FACE
# ===========================================

def upload_to_huggingface(
    output_dir: str = OUTPUT_DIR,
    repo_id: str = HF_DATASET_REPO,
    token: str = HF_TOKEN
) -> bool:
    """
    Faz upload da pasta completa de ficheiros Parquet para o Hugging Face.
    
    OTIMIZADO: Usa upload_folder para fazer tudo num √∫nico commit,
    evitando o limite de rate (128 commits/hora no plano gratuito).
    
    Args:
        output_dir: Diret√≥rio local com os ficheiros Parquet
        repo_id: ID do reposit√≥rio no formato "username/repo-name"
        token: Token de autentica√ß√£o do Hugging Face
        
    Returns:
        bool: True se o upload foi bem sucedido, False caso contr√°rio
        
    Estrutura no Hugging Face:
        repo/
        ‚îî‚îÄ‚îÄ data/
            ‚îú‚îÄ‚îÄ 00.parquet
            ‚îú‚îÄ‚îÄ 01.parquet
            ‚îî‚îÄ‚îÄ ...
    """
    logger.info(f"‚òÅÔ∏è A iniciar upload para Hugging Face...")
    logger.info(f"   Reposit√≥rio: {repo_id}")
    logger.info(f"   Pasta local: {output_dir}")
    
    try:
        # Autenticar no Hugging Face
        login(token=token)
        logger.info("   ‚úÖ Autentica√ß√£o bem sucedida")
        
        # Criar inst√¢ncia da API
        api = HfApi()
        
        # Verificar se o reposit√≥rio existe, se n√£o, criar
        try:
            api.repo_info(repo_id=repo_id, repo_type="dataset")
            logger.info(f"   ‚úÖ Reposit√≥rio encontrado")
        except Exception:
            logger.info(f"   üìÅ A criar novo reposit√≥rio...")
            api.create_repo(
                repo_id=repo_id,
                repo_type="dataset",
                private=False,  # P√∫blico para que a API possa aceder
                exist_ok=True
            )
            logger.info(f"   ‚úÖ Reposit√≥rio criado")
        
        # Contar ficheiros a enviar
        parquet_files = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]
        logger.info(f"   üì¶ A preparar upload de {len(parquet_files)} ficheiros...")
        
        # Upload da pasta completa num √∫nico commit
        # Isto evita o limite de 128 commits/hora!
        api.upload_folder(
            folder_path=output_dir,
            path_in_repo="data",
            repo_id=repo_id,
            repo_type="dataset",
            commit_message=f"Update dataset: {len(parquet_files)} partitions"
        )
        
        logger.info(f"‚úÖ Upload completo! {len(parquet_files)} ficheiros enviados.")
        logger.info(f"   üîó https://huggingface.co/datasets/{repo_id}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro no upload: {str(e)}")
        return False


# ===========================================
# CRIA√á√ÉO DO FICHEIRO DE METADADOS
# ===========================================

def create_metadata_file(partition_files: Dict[str, str], output_dir: str = OUTPUT_DIR) -> str:
    """
    Cria um ficheiro JSON com metadados do dataset.
    
    Este ficheiro ajuda a API a saber quais parti√ß√µes existem
    e cont√©m estat√≠sticas √∫teis.
    
    Args:
        partition_files: Dicion√°rio {prefixo: caminho_local}
        output_dir: Diret√≥rio de sa√≠da
        
    Returns:
        str: Caminho do ficheiro de metadados
    """
    import json
    
    # Calcular estat√≠sticas
    total_records = 0
    total_emails = 0
    total_phones = 0
    partition_stats = {}
    
    for prefix, file_path in partition_files.items():
        df = pd.read_parquet(file_path)
        count = len(df)
        total_records += count
        
        # Contar por tipo se a coluna existir
        if 'type' in df.columns:
            email_count = len(df[df['type'] == 'email'])
            phone_count = len(df[df['type'] == 'phone'])
            total_emails += email_count
            total_phones += phone_count
            partition_stats[prefix] = {
                "total": count,
                "emails": email_count,
                "phones": phone_count
            }
        else:
            partition_stats[prefix] = {"total": count}
    
    # Criar metadados com a NOVA ESTRUTURA
    metadata = {
        "version": "2.0.0",  # Vers√£o atualizada para nova estrutura
        "generated_at": datetime.now().isoformat(),
        "prefix_length": PREFIX_LENGTH,
        "compression": PARQUET_COMPRESSION,
        "schema": {
            "columns": [
                {"name": "hash", "type": "string", "description": "SHA-256 do email/phone normalizado"},
                {"name": "type", "type": "string", "description": "Tipo de dado: 'email' ou 'phone'"},
                {"name": "breach_name", "type": "string", "description": "Nome do breach"},
                {"name": "breach_date", "type": "string", "description": "Data do breach (YYYY-MM-DD)"},
                {"name": "has_password", "type": "boolean", "description": "Password foi exposta?"},
                {"name": "has_ip", "type": "boolean", "description": "IP foi exposto?"},
                {"name": "has_username", "type": "boolean", "description": "Username foi exposto?"},
                {"name": "has_credit_card", "type": "boolean", "description": "Cart√£o de cr√©dito foi exposto?"},
                {"name": "has_history", "type": "boolean", "description": "Hist√≥rico foi exposto?"}
            ]
        },
        "statistics": {
            "total_records": total_records,
            "total_emails": total_emails,
            "total_phones": total_phones,
            "total_partitions": len(partition_files)
        },
        "partitions": partition_stats
    }
    
    # Guardar ficheiro
    metadata_path = os.path.join(output_dir, "metadata.json")
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üìã Metadados guardados em: {metadata_path}")
    logger.info(f"   Total: {total_records:,} registos")
    logger.info(f"   Emails: {total_emails:,}")
    logger.info(f"   Telefones: {total_phones:,}")
    
    return metadata_path


def upload_metadata(metadata_path: str, repo_id: str = HF_DATASET_REPO, token: str = HF_TOKEN):
    """
    Faz upload do ficheiro de metadados para o Hugging Face.
    """
    api = HfApi()
    
    api.upload_file(
        path_or_fileobj=metadata_path,
        path_in_repo="data/metadata.json",
        repo_id=repo_id,
        repo_type="dataset",
        commit_message="Update metadata.json"
    )
    
    logger.info("üìã Metadados enviados para o Hugging Face")


# ===========================================
# LIMPEZA DE FICHEIROS TEMPOR√ÅRIOS
# ===========================================

def cleanup_temp_files(output_dir: str = OUTPUT_DIR):
    """
    Remove ficheiros tempor√°rios ap√≥s o upload.
    
    Args:
        output_dir: Diret√≥rio a limpar
    """
    import shutil
    
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
        logger.info(f"üßπ Ficheiros tempor√°rios removidos: {output_dir}")


# ===========================================
# FUN√á√ÉO PRINCIPAL
# ===========================================

def main():
    """
    Fun√ß√£o principal que orquestra todo o processo de atualiza√ß√£o.
    
    Fluxo:
    1. Validar configura√ß√£o
    2. Gerar/obter dados
    3. Particionar por prefixo
    4. Criar metadados
    5. Upload para Hugging Face
    6. Limpeza
    """
    print("\n" + "="*60)
    print("üëÅÔ∏è  EYE WEB UPDATER ‚Äî Breach Dataset Pipeline")
    print("="*60)
    
    # Mostrar configura√ß√£o
    print_config_summary()
    
    # 1. Validar configura√ß√£o
    logger.info("üîç A validar configura√ß√£o...")
    is_valid, message = validate_config()
    
    if not is_valid:
        logger.error(f"‚ùå Configura√ß√£o inv√°lida:\n{message}")
        sys.exit(1)
    
    logger.info(message)
    
    # 2. Gerar dados sint√©ticos (em produ√ß√£o, seria obter dados reais)
    logger.info("\n" + "-"*40)
    logger.info("FASE 1: Gera√ß√£o de Dados")
    logger.info("-"*40)
    
    df = generate_synthetic_dataset()
    
    # 3. Particionar dataset
    logger.info("\n" + "-"*40)
    logger.info("FASE 2: Particionamento")
    logger.info("-"*40)
    
    partition_files = partition_dataset(df)
    
    # 4. Criar metadados
    logger.info("\n" + "-"*40)
    logger.info("FASE 3: Metadados")
    logger.info("-"*40)
    
    metadata_path = create_metadata_file(partition_files)
    
    # 5. Upload para Hugging Face
    logger.info("\n" + "-"*40)
    logger.info("FASE 4: Upload")
    logger.info("-"*40)
    
    # Usa upload_folder para evitar limite de rate (128 commits/hora)
    success = upload_to_huggingface(OUTPUT_DIR)
    
    if success:
        upload_metadata(metadata_path)
    
    # 6. Limpeza (opcional - comentar se quiseres manter os ficheiros)
    # cleanup_temp_files()
    
    # Resumo final
    print("\n" + "="*60)
    if success:
        print("‚úÖ ATUALIZA√á√ÉO CONCLU√çDA COM SUCESSO!")
        print(f"   Dataset dispon√≠vel em:")
        print(f"   https://huggingface.co/datasets/{HF_DATASET_REPO}")
    else:
        print("‚ùå ATUALIZA√á√ÉO FALHOU - Verificar logs acima")
    print("="*60 + "\n")
    
    return 0 if success else 1


# ===========================================
# PONTO DE ENTRADA
# ===========================================

if __name__ == "__main__":
    sys.exit(main())
